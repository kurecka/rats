{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "from scipy.stats import t\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data\n",
    "output_path = Path('../outputs/20240609-192217')\n",
    "\n",
    "data = pd.read_csv(output_path / 'results.csv')\n",
    "data.fillna(0, inplace=True)\n",
    "\n",
    "data['algorithm'] = data.agent + '_t' + data.time_limit.astype(str)\n",
    "data.loc[data.agent == 'LP', 'algorithm'] = 'LP'\n",
    "data['benchmark'] = data.env + '_' + data.instance + '_sp' + data.slide_prob.astype(str) + '_tp' + data.trap_prob.astype(str)\n",
    "\n",
    "data['feasible'] = data.penalty_mean <= data.c\n",
    "\n",
    "data['t'] = (data['penalty_mean'] - data['c']) * np.sqrt(data['repetitions']) / data['penalty_std']\n",
    "data.loc[data['penalty_std'] == 0, 't'] = 0\n",
    "data['feasible_low'] = data['t'] <= t(df=data['repetitions']-1).ppf(0.95)\n",
    "data['feasible_high'] = data['t'] <= t(df=data['repetitions']-1).ppf(0.05)\n",
    "data.loc[data.agent == 'LP', 'feasible_low'] = True\n",
    "data.loc[data.agent == 'LP', 'feasible_high'] = True\n",
    "\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lp_data = data.loc[data.agent == 'LP']\n",
    "\n",
    "# Add LP accurate values\n",
    "real_values = lp_data[['reward_mean', 'benchmark', 'c']]\n",
    "\n",
    "extended_data = data.merge(real_values, on=['benchmark', 'c'], suffixes=('', '_real'))\n",
    "\n",
    "m = extended_data['reward_mean'].min()\n",
    "\n",
    "extended_data['reward_mean'] -= m\n",
    "extended_data['reward_mean_real'] += 0.00001\n",
    "\n",
    "extended_data.sort_values(by=['benchmark', 'algorithm', 'c'], inplace=True)\n",
    "extended_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# minimum of reward_mean and reward_mean_real\n",
    "def get_table(data):\n",
    "    data['min_reward_mean'] = data[['reward_mean', 'reward_mean_real']].min(axis=1)\n",
    "    data['cvm'] = data['feasible'] * data['min_reward_mean']\n",
    "    data['cvl'] = data['feasible_low'] * data['min_reward_mean']\n",
    "    data['cvh'] = data['feasible_high'] * data['min_reward_mean']\n",
    "\n",
    "    for m in ['cvm', 'cvl', 'cvh']:\n",
    "        data[m] /= data.groupby('algorithm')[m].mean().max()\n",
    "\n",
    "    data['cvmn'] = data['feasible'] * data['min_reward_mean'] / (data['reward_mean_real'])\n",
    "    data['cvln'] = data['feasible_low'] * data['min_reward_mean'] / (data['reward_mean_real'])\n",
    "    data['cvhn'] = data['feasible_high'] * data['min_reward_mean'] / (data['reward_mean_real'])\n",
    "\n",
    "    return data.groupby(['algorithm'])[[\n",
    "        'feasible_low', 'feasible', 'feasible_high',\n",
    "        'cvl', 'cvm', 'cvh',\n",
    "        'cvln', 'cvmn', 'cvhn',\n",
    "    ]].mean().sort_values(by='feasible', ascending=False)\n",
    "\n",
    "tables = {\n",
    "    'table.csv': get_table(extended_data),\n",
    "}\n",
    "for env in extended_data.env.unique():\n",
    "    tables[f'table_{env}.csv'] = get_table(extended_data.loc[extended_data.env == env])\n",
    "\n",
    "for name, table in tables.items():\n",
    "    table.to_csv(output_path / name)\n",
    "    print('\\n', name, '\\n', table)\n",
    "\n",
    "\n",
    "warnings.filterwarnings(\"default\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import warnings\n",
    "# warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# # Plot meand reward vs. c\n",
    "# # Each benchmark is a subplot\n",
    "# # Each algorithm is a line\n",
    "# # Use `reward_std` for error bars\n",
    "\n",
    "\n",
    "# for time in [t for t in extended_data['time_limit'].unique() if t >= 0]:\n",
    "#     filtered_data = extended_data[(extended_data['time_limit'] == time) | (extended_data['time_limit'] == -1)]\n",
    "#     filtered_data['reward_std'] /= np.sqrt(filtered_data['repetitions'])\n",
    "\n",
    "#     default_palette = sns.color_palette()\n",
    "#     algorithms = filtered_data['algorithm'].unique()\n",
    "#     palette = {algorithm: default_palette[i] for i, algorithm in enumerate(algorithms)}\n",
    "\n",
    "\n",
    "#     # Create a FacetGrid with the benchmark feature\n",
    "#     g = sns.FacetGrid(filtered_data, col=\"benchmark\", col_wrap=4, height=4, aspect=1.5)\n",
    "\n",
    "#     # Do not show the plot in a notebook\n",
    "#     plt.ioff()\n",
    "\n",
    "#     # Iterate through each subplot to add error bars manually\n",
    "#     for ax, (benchmark_value, subset) in zip(g.axes.flatten(), filtered_data.groupby('benchmark')):\n",
    "#         sns.lineplot(x='c', y='reward_mean', hue='algorithm', palette=palette, err_style=None, data=subset, ax=ax)\n",
    "\n",
    "#         non_feasible = subset[subset['feasible_low'] == False]\n",
    "#         sns.scatterplot(x='c', y='reward_mean', hue='algorithm', palette=palette, marker='x', s=50, data=non_feasible, ax=ax)\n",
    "#         ax.grid(True)\n",
    "\n",
    "#         for alg in algorithms:\n",
    "#             alg_data = subset[subset['algorithm'] == alg]\n",
    "#             ax.fill_between(\n",
    "#                 alg_data['c'],\n",
    "#                 alg_data['reward_mean'] - alg_data['reward_std'],\n",
    "#                 alg_data['reward_mean'] + alg_data['reward_std'],\n",
    "#                 alpha=0.2,\n",
    "#                 color=palette[alg]\n",
    "#             )\n",
    "\n",
    "#     # Adjust the titles and labels\n",
    "#     g.set_titles(col_template=\"{col_name}\")\n",
    "#     g.set_axis_labels(\"C\", \"Mean Reward\")\n",
    "\n",
    "#     g.savefig(output_path / f'mean_reward_vs_c_t{time}.svg')\n",
    "\n",
    "\n",
    "# warnings.filterwarnings(\"default\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
