
import copy
from fimdpenv import AEVEnv
import folium
from itertools import islice
from math import asin, cos, sqrt, pi

import numpy as np
import networkx as nx
import time

# defaults from their implementation
targets = ['42440465','42445916']

# Calculates distance in km from latitude and longitude of two points using
# the Haversine formula,
# see: https://stackoverflow.com/questions/27928/calculate-distance-between-two-latitude-longitude-points-haversine-formula
def distance_from_gps(long1, lat1, long2, lat2):
    R = 6371
    p = pi / 180
    a = 0.5 - cos((lat2-lat1)*p)/2 + cos(lat1*p) * cos(lat2*p) * \
                  (1-cos((long2-long1)*p))/2
    return 2 * R * asin(sqrt(a))

# states - labels (json), see above
# actions - integers, (see variable aid in fimdp/core.py)
# S - state_label, state_of_targets, decision_node
# A - index size_t
class ManhattanEnv:

    """
        capacity - starting capacity of the agent
        targets - the reachability targets, currently receive unit reward for
            hitting a state in targets
        init_state - initial state  (json label), randomize if none

        self.energy / self.capacity -> curr/max energy

        self.history -> tracks data that is recorded before calling step()
            i.e. S = (state, state_of_targets, decision_node, energy)

        periods -> dict of target + int/float

        cons_thd = threshold consumption to receive penalty after accepting order

        MDP state consists of the current state, the counters for each target, and decision node indicator
    """
    def __init__(self, targets, init_state, periods, capacity, cons_thd=10.0, radius=2.0):
        self.env = AEVEnv.AEVEnv(capacity, targets, [], init_state,
                datafile="/work/rats/rats/manhattan_res/NYC.json",
                mapfile ="/work/rats/rats/manhattan_res/NYC.graphml")

        # maps int -> ( state, energy )
        self.checkpoints = dict()
        # maps int -> histories
        self.histories = dict()

        if not init_state:
            # randomize starting state
            self.init_state = self.random_state()
        else:
            self.init_state = init_state

        self.position = self.init_state

        # last gps position
        self.last_gps = None

        self.capacity = capacity
        self.energy = capacity

        self.targets = targets
        self.periods = periods
        self.radius = radius
        self.cons_thd = cons_thd

        # decision node -> special part of state (flag) - signals that orders
        # can be accepted at this step
        self.decision_node = False

        # ctr for each target, -1 is active target (accepted order)
        self.state_of_targets = { t : self.periods[t] for t in self.targets }

        self.history = []

        # graph with latitude and longitude information, used to evaluate
        # distance from targets and plot an animation of the trajectory.
        self.G = nx.MultiDiGraph(nx.read_graphml(self.env.mapfile))
        self.geo_data = self.G.nodes(data=True)


    """
        helper methods
    """
    # translate names (json labels) to cmdp states used in functions
    def name_to_state(self, name):
        return self.env.consmdp.names_dict[name]

    # translate indices in cmdp to json labels
    def state_to_name(self, state):
        return self.env.state_to_name[state]

    # generate a random state (TODO: use for delivery targets)
    # FIXME: dummy states for actions can be generated by this, probably not
    # what we want
    def random_state(self):
        return self.state_to_name(np.random.choice(self.env.consmdp.num_states))

    """
        env interface methods
    """

    def name(self):
        return "ManhattanEnv"

    def num_actions(self):
        return len(self.possible_actions())

    """
        get the number of actions for the current state
        if in decision node, return indices to active targets

        -1 signals refusing any orders
    """
    def get_actions_for_state(self, name):
        # able to accept orders
        if ( self.decision_node ):
            return [ -1 ] + [ i for i, t in enumerate(self.targets) if self.state_of_targets[t] == 0 ]

        state_id = self.name_to_state(name)
        action_count = len(self.env.consmdp.actions_for_state(state_id))
        return list(range(action_count))

    def current_state(self):
        return self.position, self.state_of_targets, self.decision_node

    # c++ state is a tuple of (state_name, state_of_targets, decision_node)
    def possible_actions(self, state = None):

        # if called with deafult init state tuple (from c++) or None,
        # get actions for current position
        if state is None or state[0] == '':
            state_name = self.position
        else:
            state_name = state[0]
        return self.get_actions_for_state(state_name)

    def target_active(self, target):
        return self.state_of_targets[target] == -1

    def get_action(self, idx):
        return possible_actions()[idx]

    # TODO: not implemented, does not take into account the dummy nodes in
    # original benchmark.
    def outcome_probabilities(self, name, action):

        dist = dict()

        state_id = self.name_to_state(name)

        # ActionData... source target, consumption, distr
        action_iterator = self.env.consmdp.actions_for_state(state_id)

        # the iterator does not have random access, pull out the action like this
        action_data = next(islice(action_iterator, action, None))

        for x in action_data.distr.keys():
            dist[self.state_to_name(x)] = float(action_data.distr[x])

        return dist
    # decreases counters on orders by _cons_, any orders that reach zero and
    # are not sufficiently close (< self.radius), are refreshed as well
    # returns a flag signalizing that the agent can accept an order
    def decrease_ctrs(self, cons):
        orders_available = False
        for t in self.targets:
            if self.target_active(t):
                continue

            self.state_of_targets[t] -= cons

            if self.state_of_targets[t] <= 0:
                dist = self.distance_to_target(t)
                new_value = self.periods[t]

                if dist <= self.radius:
                    new_value = 0
                    orders_available = True

                self.state_of_targets[t] = new_value

        return orders_available


    def reload_ctrs(self):
        for t in self.targets:
            if self.state_of_targets[t] == 0:
                self.state_of_targets[t] = self.periods[t]


    def distance_to_target(self, target):
        # some states do not have longitude and latitude information in the
        # graph file, need to work around this
        if self.position not in self.geo_data:

            # check if last known gps position is initialized
            if self.last_gps is None:
                return []

            # use last known gps position
            else:
                state_data = self.geo_data[self.last_gps]

        # use the current position
        else:
            state_data = self.geo_data[self.position]
            self.last_gps = self.position

        state_lat = float(state_data['lat'])
        state_lon = float(state_data['lon'])

        target_data = self.geo_data[target]
        target_lat = float(target_data['lat'])
        target_lon = float(target_data['lon'])

        return distance_from_gps(state_lon, state_lat, target_lon, target_lat)


    def play_action(self, action):

        # record relevant information (used for animating trajectory)
        self.history.append( (self.position, self.state_of_targets, self.decision_node) )


        # if in decision node, handle (potential) acceptance of an order
        if self.decision_node:
            self.decision_node = False

            # action is an index to targets
            if action != -1:
                # accept target
                self.state_of_targets[self.targets[action]] = -1
            self.reload_ctrs()
            return (self.position, self.state_of_targets, self.decision_node), 0, 0, self.is_over()

        # otherwise, proceed by moving in the underlying cmdp, recording
        # reward/penalty and adjusting periods of orders

        # get linked list of actions from cmdp
        state_id = self.name_to_state(self.position)
        action_iterator = self.env.consmdp.actions_for_state(state_id)

        # get the action from iterator
        action_data = next(islice(action_iterator, action, None))

        # get successor
        next_state = np.random.choice(list(action_data.distr.keys()),
                                      p=list(action_data.distr.values()))

        # skip dummy state, record penalty
        action_iterator = self.env.consmdp.actions_for_state(next_state)
        action_data = next(islice(action_iterator, 0, None))

        next_state = next(iter(action_data.distr))
        self.position = self.state_to_name(next_state)

        # if limited energy budget is set, decrease the current energy
        if self.capacity > 0:
            self.energy -= action_data.cons

        # decrease counters for targets
        # TODO: maybe clip the consumption to a predefined range like [1, 2, 3]
        self.decision_node = self.decrease_ctrs(action_data.cons)

        # penalize if consumption is past threshold and some target is active
        penalty = (action_data.cons >= self.cons_thd) * ( len([1 for t in self.targets if self.state_of_targets[t] == -1]) > 0 )

        # reward if target is reached
        reward = 0
        if (self.position in self.targets) and (self.state_of_targets[self.position] == -1):
            reward = 1
            self.state_of_targets[self.position] = self.periods[self.position]

        return (self.position, self.state_of_targets, self.decision_node), float(reward), float(penalty), self.is_over()


    # if capacity is zero the environment does not terminate
    def is_over(self):
        return self.capacity > 0 and self.energy <= 0

    def make_checkpoint(self, checkpoint_id):
        history_copy = copy.deepcopy(self.history)
        counter_copy = copy.deepcopy(self.state_of_targets)

        self.histories[checkpoint_id] = history_copy
        self.checkpoints[checkpoint_id] = (self.position, self.last_gps, self.energy, counter_copy, self.decision_node)

    def restore_checkpoint(self, checkpoint_id):
        self.history = self.histories[checkpoint_id]
        self.position, self.last_gps, self.energy, self.state_of_targets, self.decision_node = self.checkpoints[checkpoint_id]

    def reset(self):
        self.energy = self.capacity
        self.position = self.init_state
        self.state_of_targets = { t : self.periods[t] for t in self.targets }
        self.decision_node = False

        self.histories.clear()
        self.checkpoints.clear()
        self.history = []

    """
        taken from AEVEnv

        call after final execution of the policy, i.e.
        after calling play_action() as many times as you wish
    """

    # TODO:
    # need to call reset beforehand to clear the history, afterwards this
    # should work fine.
    # interval signals number of frames between each animation plot
    def animate_simulation(self, interval=100, filename="map.html"):
        """
        Obtain the animation of a simulation instance where the agent reaches
        the target state from the initial state using assigned counterstrategy
        """
        targets = self.targets
        init_state = self.init_state

        def is_int(s):
            try:
                int(s)
                return True
            except ValueError:
                return False


        # Load NYC Geodata
        for _, _, data in self.G.edges(data=True, keys=False):
            data['time_mean'] = float(data['time_mean'])
        for _, data in self.geo_data:
            data['lat'] = float(data['lat'])
            data['lon'] = float(data['lon'])

        for target in targets:
            if target not in list(self.G.nodes):
                targets.remove(target)

        trajectory = []

        # filter dummy states
        for position, targets, decision  in self.history:
            if position not in list(self.G.nodes):
                pass
            else:
                trajectory.append( (position, targets, decision) )

        # create baseline map
        nodes_all = {}
        for node in self.G.nodes.data():
            name = str(node[0])
            point = [node[1]['lat'], node[1]['lon']]
            nodes_all[name] = point
        global_lat = []; global_lon = []
        for name, point in nodes_all.items():
            global_lat.append(point[0])
            global_lon.append(point[1])
        min_point = [min(global_lat), min(global_lon)]
        max_point =[max(global_lat), max(global_lon)]
        m = folium.Map(zoom_start=1, tiles='cartodbpositron')
        m.fit_bounds([min_point, max_point])

        # add initial state, reload states and target states
        folium.CircleMarker(location=[self.G.nodes[init_state]['lat'], self.G.nodes[init_state]['lon']],
                        radius= 3,
                        popup = 'initial state',
                        color='green',
                        fill_color = 'green',
                        fill_opacity=1,
                        fill=True).add_to(m)

        for node in targets:
            folium.CircleMarker(location=[self.G.nodes[node]['lat'], self.G.nodes[node]['lon']],
                        radius= 3,
                        popup = 'target state',
                        color="red",
                        fill_color = "red",
                        fill_opacity=1,
                        fill=True).add_to(m)
        # Baseline time

        t = time.time()
        path = list(zip(trajectory[:-1], trajectory[1:]))
        lines = []
        current_positions = []
        for pair in path:

            # pull out positions from the history tuple
            pos1, pos2 = pair[0][0], pair[1][0]

            t_edge = 1
            lines.append(dict({'coordinates':
                [[self.G.nodes[pos1]['lon'], self.G.nodes[pos1]['lat']],
                [self.G.nodes[pos2]['lon'], self.G.nodes[pos2]['lat']]],
                'dates': [time.strftime('%Y-%m-%dT%H:%M:%S', time.localtime(t)),
                           time.strftime('%Y-%m-%dT%H:%M:%S', time.localtime(t+t_edge))],
                           'color':'black'}))
            current_positions.append(dict({'coordinates':[self.G.nodes[pos2]['lon'], self.G.nodes[pos2]['lat']],
                        'dates': [time.strftime('%Y-%m-%dT%H:%M:%S', time.localtime(t+t_edge))]}))
            t = t+t_edge

        features = [{'type': 'Feature',
                     'geometry': {
                                'type': 'LineString',
                                'coordinates': line['coordinates'],
                                 },
                     'properties': {'times': line['dates'],
                                    'style': {'color': line['color'],
                                              'weight': line['weight'] if 'weight' in line else 2
                                             }
                                    }
                     }
                for line in lines]

        positions = [{
            'type': 'Feature',
            'geometry': {
                        'type':'Point',
                        'coordinates':position['coordinates']
                        },
            'properties': {
                'times': position['dates'],
                'style': {'color' : 'white'},
                'icon': 'circle',
                'iconstyle':{
                    'fillColor': 'white',
                    'fillOpacity': 1,
                    'stroke': 'true',
                    'radius': 2
                }
            }
        }
         for position in current_positions]
        data_lines = {'type': 'FeatureCollection', 'features': features}
        data_positions = {'type': 'FeatureCollection', 'features': positions}
        folium.plugins.TimestampedGeoJson(data_lines,  transition_time=interval,
                               period='PT1S', add_last_point=False, date_options='mm:ss', duration=None).add_to(m)

        m.save(filename)

if __name__ == "__main__":

    # pretty close starting state for easy testing
    init_state = '42429690'

    # three states next to each other for default orders
    targets = ['42455666', '42442977', '596775930']

    # periods
    periods = { target : 50 for target in targets }

    # higher period for last target
    periods[targets[-1]] = 100
    x = ManhattanEnv(targets, init_state, periods, capacity)

    for i in range(30):
        print(x.current_state())
        print(x.possible_actions())
        a = x.possible_actions()[-1]
        s, r, p, o = x.play_action(a)
        print(r, p, o)
    x.animate_simulation(interval=100)
